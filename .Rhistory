teamRanks2$`Winnipeg Jets` <- NA
teamRanks <- teamRanks %>%
select(1:31,33:32)
teamRanks2 <- teamRanks2 %>%
select(1:29,31, 33, 30, 32)
teamRanks3 <- rbind(teamRanks, teamRanks2)
# Join ranks to main dataframe -------------------------------------------------------------------------
# gather team ranks dataframe
teamRanksGathered <- teamRanks3 %>%
select(-"home") %>%
gather(targetTeam, rank, -date)
#table(teamRanksGathered$targetTeam)
# check results
#glimpse(teamRanksGathered)
#glimpse(teamRanks3)
# left join rank onto the main dataframe
MainDataFrameALL <- matches_5 %>%
left_join(teamRanksGathered, by = c("date" = "date", "target_team" = "targetTeam"))
# check result
# glimpse(MainDataFrameALL)
# Rearrange Main Dataframe to have target and opponent relevant predictors with "Outcome Ready for Models --------
# duplicate main dataframe
MainDataFrameAllDuplicate <- MainDataFrameALL
# rejoin to original with left join on opponent/targetteam and date
MainDataFrameFinal <- MainDataFrameALL %>%
left_join(MainDataFrameAllDuplicate, by = c("opponent_team" = "target_team", "date" = "date"))
# glimpse(MainDataFrameFinal)
#x is target team related and y is opponent team related in variable names
# Arrange dataframe for model ---------------------------------------------------------------------------------------------
# ratios, delete unwanted variables, differences etc
# names(MainDataFrameFinal)
#glimpse(MainDataFrameFinal)
MainDataFrameFinal$seasonPointsDifference <- MainDataFrameFinal$offset_season_points.x - MainDataFrameFinal$offset_season_points.y
MainDataFrameFinal$seasonPlusMinusDifference <- MainDataFrameFinal$offset_cumsum_plusminus.x - MainDataFrameFinal$offset_cumsum_plusminus.y
# recode home and away predictor
MainDataFrameFinal$target_homeAway.x <- revalue(MainDataFrameFinal$target_homeAway.x, c("Home" = 1, "Away" = 0))
MainDataFrameFinal$target_homeAway.y <- revalue(MainDataFrameFinal$target_homeAway.y, c("Home" = 1, "Away" = 0))
# glimpse(MainDataFrameFinal)
# Write to csv and final adjustments---------------------------------------------------------------------------
# drop coordinate columns
#names(MainDataFrameFinal)
MainDataFrameFinal2 <- MainDataFrameFinal %>%
select(-c(8, 12:13, 15, 28:31, 35:45, 47, 60:63))
# make sure games only recorded once using target outcome correct
MainDataFrameFinal2$original_game <- ifelse((MainDataFrameFinal$target_goals.x - MainDataFrameFinal$opponent_goals.x) == MainDataFrameFinal$target_goal_difference.x, 1, 0)
MainDataFrameFinal2 <- MainDataFrameFinal2 %>%
filter(original_game == 1) %>%
select(-45)
#glimpse(MainDataFrameFinal2)
# adjust rank - split dataframe then remerge
rank_adjust <- split(MainDataFrameFinal2, list(MainDataFrameFinal2$target_team, MainDataFrameFinal2$season.x))
# Remove any empty data frames.... there are 3 due to teams changes i.e. vegas knights didn't exist in 2014/15 season
rank_adjust <-  rank_adjust[sapply(rank_adjust, function(x) dim(x)[1]) > 0]
rank_adjust <- lapply(rank_adjust, function(df) {df$offset_rank.x <- offset_column(df, "rank.x", 1); df})
rank_adjust <- lapply(rank_adjust, function(df) {df$offset_rank.y <- offset_column(df, "rank.y", 1); df})
MainDataFrameFinal3 <- bind_rows(rank_adjust)
MainDataFrameFinal3$rankDifference <- MainDataFrameFinal3$offset_rank.x - MainDataFrameFinal3$offset_rank.y
MainDataFrameFinal3 <- drop_column(MainDataFrameFinal3, "rank.x")
MainDataFrameFinal3 <- drop_column(MainDataFrameFinal3, "rank.y")
#glimpse(MainDataFrameFinal3)
# too big to write to excel workbook with XL connect
write.csv(MainDataFrameFinal3, 'df_engineered.csv')
writeWorksheet(wb, data = MainDataFrameFinal3, sheet = "df_engineered")
saveWorkbook(wb)
# Naive Bayes & GLM Model
setwd("C:/Users/jaybl/OneDrive/DS_Projects/NHL_Prediction_With_Pre_Match_data/")
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(caret)
library(vip)
library(corrplot)
# Load and Inpsect Data -----------------------------------------------------------------------
data <- read.csv("df_engineered.csv") %>% select(-1) # drop index
glimpse(data)
# make all columns integers
data2 <- data %>%
mutate(extra_time.x = as.factor(ifelse(is.na(extra_time.x), 0, extra_time.x))) %>%
mutate(days_since_home.y = ifelse(is.na(days_since_home.y), 0, days_since_home.y )) %>%
mutate(offset_rank.y = ifelse(is.na(offset_rank.y), 0, offset_rank.y)) %>%
mutate(offset_games_since_home_match.x = ifelse(is.na(offset_games_since_home_match.x), 0, offset_games_since_home_match.x)) %>%
mutate(rankDifference = ifelse(is.na(rankDifference), 0, rankDifference)) %>%
mutate(offset_rank.x = ifelse(is.na(offset_rank.x), 0, offset_rank.x)) %>%
filter(matchcounter.x >= 10) %>% # remove the first 9 games from every team for every season
select(-c("opponent_team", "target_team", "original_game", "date", "target_goals.x", "opponent_goals.x",
"target_goal_difference.x")) %>%
mutate_if(is.numeric, as.integer, na.rm = FALSE)
# correlation plot & splitting----------------------------------------------------------------------------
glimpse(data2)
sum(is.na(data2))
cor_matrix <- data2 %>% select(-c(1:4)) %>% cor()
cor_matrix
plot(cor_matrix)
cor_plot <- cor_matrix %>%
corrplot()
training <- data2 %>%
filter(season.x != "2019/2020") %>%
select(-"season.x")
testing <- data2 %>%
filter(season.x == "2019/2020") %>%
select(-c("target_outcome.x", "season.x"))
testing_labels <- data2 %>%
filter(season.x == "2019/2020")
testing_labels <- testing_labels$target_outcome.x
# Models ---------------------------------------------------------------------------------------
fitControl <- trainControl(
method = "repeatedcv",
repeats = 10,
number = 10)
set.seed(825)
nb1 <- train(target_outcome.x ~ ., data = training,
method = "nb",
trControl = fitControl)
warnings()
predictions_nb1 <- predict(nb1, testing)
warnings()
confusionMatrix(predictions_nb1, factor(testing_labels))
predictions_nb1 <- predict(nb1, testing, type = 'prob')
predictions_nb1
predictions_nb2 <- predict(nb1, testing, type = 'prob')
predictions_nb2$actual <-testing_labels
predictions_nb2
predictions_nb3 <- predictions_nb2 %>%
filter(Win > 0.7)
predictions_nb3
table(predictions_nb3$actual)
predictions_nb3 <- predictions_nb2 %>%
filter(Win > 0.9)
table(predictions_nb3$actual)
predictions_nb1 <- predict(nb1, testing)
vip(predictions_glm1, num_features = 25, geom = 'col')
# glm
predictions_glm1 <- predict(glm1, testing)
glm1 <- train(target_outcome.x ~ ., data = training,
method = "glm",
family = "binomial",
trControl = fitControl)
warnings()
# glm
predictions_glm1 <- predict(glm1, testing)
confusionMatrix(predictions_glm1, factor(testing_labels))
vip(predictions_glm1, num_features = 25, geom = 'col')
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(ranger)   # a c++ implementation of random forest
library(h2o)
library(vip)
hyper_grid <- expand.grid(
mtry = floor(n_preds * c(.05, .15, .25, .333, .4)),
min.node.size = c(1, 3, 5, 10),
replace = c(TRUE, FALSE),
sample.fraction = c(.5, .63, .8),
rmse = NA
)
n_preds <- length(setdiff(names(training), "target_outcome.x"))
hyper_grid <- expand.grid(
mtry = floor(n_preds * c(.05, .15, .25, .333, .4)),
min.node.size = c(1, 3, 5, 10),
replace = c(TRUE, FALSE),
sample.fraction = c(.5, .63, .8),
rmse = NA
)
# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
# fit model for ith hyperparameter combination
fit <- ranger(
formula         = target_outcome.x ~ .,
data            = training,
num.trees       = n_preds * 10,
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$min.node.size[i],
replace         = hyper_grid$replace[i],
sample.fraction = hyper_grid$sample.fraction[i],
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
importance = 'impurity'
)
# export OOB error
hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
# assess top 10 models
hyper_grid %>%
arrange(rmse) %>%
mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
head(10)
data_rf1 <- ranger(
target_outcome.x ~ .,
data = training,
mtry = floor(n_preds / 3),
seed = 123
)
# Get OOb RMSE
(default_rmse <- sqrt(data_rf1$prediction.error))
hyper_grid <- expand.grid(
mtry = floor(n_preds * c(.05, .15, .25, .333, .4)),
min.node.size = c(1, 3, 5, 10),
replace = c(TRUE, FALSE),
sample.fraction = c(.5, .63, .8),
rmse = NA
)
# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
# fit model for ith hyperparameter combination
fit <- ranger(
formula         = target_outcome.x ~ .,
data            = training,
num.trees       = n_preds * 10,
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$min.node.size[i],
replace         = hyper_grid$replace[i],
sample.fraction = hyper_grid$sample.fraction[i],
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
importance = 'impurity'
)
# export OOB error
hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
# assess top 10 models
hyper_grid %>%
arrange(rmse) %>%
mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
head(10)
data_rf1 <- ranger(
formula         = target_outcome.x ~ .,
data            = training,
num.trees       = n_preds * 10,
mtry            = 9,
min.node.size   = 1,
replace         = TRUE,
sample.fraction = 0.5,
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
importance = 'impurity'
)
predict(data_rf1, testing_labels)
vip(data_rf1, num_features = 25, geom = 'col')
vip(data_rf1, num_features = 40, geom = 'col')
vip(data_rf1, num_features = 37, geom = 'col')
training <- data2 %>%
training <- data2 %>%
filter(season.x != "2019/2020") %>%
select(c("rankDifference", "target_outcome.x"))
testing <- data2 %>%
filter(season.x == "2019/2020") %>%
select(c("rankDifference"))
testing_labels <- data2 %>%
filter(season.x == "2019/2020")
testing_labels <- testing_labels$target_outcome.x
n_preds <- length(setdiff(names(training), "target_outcome.x"))
data_rf1 <- ranger(
target_outcome.x ~ .,
data = training,
mtry = floor(n_preds / 3),
seed = 123
)
# Get OOb RMSE
(default_rmse <- sqrt(data_rf1$prediction.error))
hyper_grid <- expand.grid(
mtry = floor(n_preds * c(.05, .15, .25, .333, .4)),
min.node.size = c(1, 3, 5, 10),
replace = c(TRUE, FALSE),
sample.fraction = c(.5, .63, .8),
rmse = NA
)
# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
# fit model for ith hyperparameter combination
fit <- ranger(
formula         = target_outcome.x ~ .,
data            = training,
num.trees       = n_preds * 10,
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$min.node.size[i],
replace         = hyper_grid$replace[i],
sample.fraction = hyper_grid$sample.fraction[i],
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
importance = 'impurity'
)
# export OOB error
hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
# assess top 10 models
hyper_grid %>%
arrange(rmse) %>%
mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
head(10)
n_preds <- length(setdiff(names(training), "target_outcome.x"))
training <- data2 %>%
filter(season.x != "2019/2020") %>%
select(c("rankDifference", "target_outcome.x"))
testing <- data2 %>%
filter(season.x == "2019/2020") %>%
select(c("rankDifference"))
testing_labels <- data2 %>%
filter(season.x == "2019/2020")
testing_labels <- testing_labels$target_outcome.x
n_preds <- length(setdiff(names(training), "target_outcome.x"))
data_rf1 <- ranger(
target_outcome.x ~ .,
data = training,
mtry = floor(n_preds / 3),
seed = 123
)
# Get OOb RMSE
(default_rmse <- sqrt(data_rf1$prediction.error))
hyper_grid <- expand.grid(
mtry = floor(n_preds * c(.05, .15, .25, .333, .4)),
min.node.size = c(1, 3, 5, 10),
replace = c(TRUE, FALSE),
sample.fraction = c(.5, .63, .8),
rmse = NA
)
# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
# fit model for ith hyperparameter combination
fit <- ranger(
formula         = target_outcome.x ~ .,
data            = training,
num.trees       = n_preds * 10,
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$min.node.size[i],
replace         = hyper_grid$replace[i],
sample.fraction = hyper_grid$sample.fraction[i],
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
importance = 'impurity'
)
# export OOB error
hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
# assess top 10 models
hyper_grid %>%
arrange(rmse) %>%
mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
head(10)
data_rf1 <- ranger(
formula         = target_outcome.x ~ .,
data            = training,
num.trees       = n_preds * 10,
mtry            = 9,
min.node.size   = 1,
replace         = TRUE,
sample.fraction = 0.5,
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
importance = 'impurity'
)
data_rf1 <- ranger(
formula         = target_outcome.x ~ .,
data            = training,
num.trees       = n_preds * 10,
mtry            = 0,
min.node.size   = 1,
replace         = FALSE,
sample.fraction = 0.8,
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
importance = 'impurity'
)
predict(data_rf1, testing_labels)
vip(data_rf1, num_features = 37, geom = 'col')
glimpse(data)
training <- data2 %>%
filter(season.x != "2019/2020") %>%
select(c("rankDifference", "target_outcome.x", "V19.x", "V23.x", "roadTrip_cumsum.x",
"V19.y", "V23.y", "roadTrip_cumsum.y"))
testing <- data2 %>%
filter(season.x == "2019/2020") %>%
select(c("rankDifference", "V19.x", "V23.x", "roadTrip_cumsum.x",
"V19.y", "V23.y", "roadTrip_cumsum.y"))
testing_labels <- data2 %>%
filter(season.x == "2019/2020")
testing_labels <- testing_labels$target_outcome.x
n_preds <- length(setdiff(names(training), "target_outcome.x"))
data_rf1 <- ranger(
target_outcome.x ~ .,
data = training,
mtry = floor(n_preds / 3),
seed = 123
)
# Get OOb RMSE
(default_rmse <- sqrt(data_rf1$prediction.error))
hyper_grid <- expand.grid(
mtry = floor(n_preds * c(.05, .15, .25, .333, .4)),
min.node.size = c(1, 3, 5, 10),
replace = c(TRUE, FALSE),
sample.fraction = c(.5, .63, .8),
rmse = NA
)
# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
# fit model for ith hyperparameter combination
fit <- ranger(
formula         = target_outcome.x ~ .,
data            = training,
num.trees       = n_preds * 10,
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$min.node.size[i],
replace         = hyper_grid$replace[i],
sample.fraction = hyper_grid$sample.fraction[i],
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
importance = 'impurity'
)
# export OOB error
hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
# assess top 10 models
hyper_grid %>%
arrange(rmse) %>%
mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
head(10)
data_rf1 <- ranger(
formula         = target_outcome.x ~ .,
data            = training,
num.trees       = n_preds * 10,
mtry            = 1,
min.node.size   = 10,
replace         = FALSE,
sample.fraction = 0.5,
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
importance = 'impurity'
)
vip(data_rf1, num_features = 37, geom = 'col')
training <- data2 %>%
filter(season.x != "2019/2020") %>%
select(c("rankDifference", "target_outcome.x", "V19.x", "V23.x", "roadTrip_cumsum.x",
"V19.y", "V23.y", "roadTrip_cumsum.y"))
testing <- data2 %>%
filter(season.x == "2019/2020") %>%
select(c("rankDifference", "V19.x", "V23.x", "roadTrip_cumsum.x",
"V19.y", "V23.y", "roadTrip_cumsum.y"))
testing_labels <- data2 %>%
filter(season.x == "2019/2020")
testing_labels <- testing_labels$target_outcome.x
fitControl <- trainControl(
method = "repeatedcv",
repeats = 10,
number = 10)
set.seed(825)
glm1 <- train(target_outcome.x ~ ., data = training,
method = "glm",
family = "binomial",
trControl = fitControl)
# glm
predictions_glm1 <- predict(glm1, testing)
confusionMatrix(predictions_glm1, factor(testing_labels))
vip(predictions_glm1, num_features = 25, geom = 'col')
nb1 <- train(target_outcome.x ~ ., data = training,
method = "nb",
trControl = fitControl)
warnings()
#naive bayes
predictions_nb1 <- predict(nb1, testing)
warnings()
predictions_nb2 <- predict(nb1, testing, type = 'prob')
predictions_nb2$actual <-testing_labels
predictions_nb3 <- predictions_nb2 %>%
filter(Win > 0.9)
table(predictions_nb3$actual)
predictions_nb3 <- predictions_nb2 %>%
filter(Win > 0.8)
table(predictions_nb3$actual)
predictions_nb3 <- predictions_nb2 %>%
filter(Win > 0.7)
table(predictions_nb3$actual)
predictions_nb3 <- predictions_nb2 %>%
filter(Win > 0.6)
table(predictions_nb3$actual)
confusionMatrix(predictions_nb1, factor(testing_labels))
predictions_nb3 <- predictions_nb2 %>%
filter(Loss > 0.6)
table(predictions_nb3$actual)
predictions_nb3 <- predictions_nb2 %>%
filter(Loss > 0.7)
table(predictions_nb3$actual)
predictions_nb3 <- predictions_nb2 %>%
filter(Loss > 0.5)
table(predictions_nb3$actual)
mars1 = train(target_outcome.x ~ .
, data = training
, method = 'earth'
, tuneGrid = grid
, trControl = trainControl(method = 'cv',
verboseIter = FALSE,
savePredictions = TRUE)
)
grid = expand.grid( degree = 1, nprune = 40  )
grid = expand.grid( degree = 1, nprune = 40)
mars1 = train(target_outcome.x ~ .
, data = training
, method = 'earth'
, tuneGrid = grid
, trControl = trainControl(method = 'cv',
verboseIter = FALSE,
savePredictions = TRUE)
)
# mars model
predictions_mars1 <- predict(mars1, testing)
confusionMatrix(predictions_mars1, testing_labels)
vip(mars1, num_features = 10, geom = 'col')
